{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement.\n",
    "The moderators for a Spirituality discussion forum have approached me to help them sieve out unwanted posts. The forum is focused on the subject of Spirituality but there have been frequent postings from religious groups that try to disrupt the discussion and we have received feedback from our senior forum members to remove such postings. However, there are hundreds of postings each day and we will need to develop a quick way to identify such posts and remove them quickly.\n",
    "\n",
    "### Stakeholders:\n",
    "Moderators of the Spirituality forum.\n",
    "\n",
    "### Approach:\n",
    "Using classification models such as Logistic Regression, Naive Bayes, RandomForest, correctly predict whether a post belongs to Spirituality or Religion. Data will be scrapped from reddit.com, using subreddits /r/spirituality and /r/religion. A model will be trained to predict Spirituality as the positive class and top prediction features will also be identified.\n",
    "\n",
    "### Measure of Success:\n",
    "The classification model would be assessed on its test accuracy and specificity score. The model should reduce false positives as much as possible. We do not want posts related to Religion getting classified as Spirituality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the problem of posting of religious content in our Spirituality forum, we built a Classification Model using postings from reddit.com/r/spirituality/ and reddit.com/r/religion/. A total of about 1300 posts from each subreddits were collected for training and testing the model.\n",
    "\n",
    "A range of classification models including Logistic Regression, Naive-Bayes Multinomial and RandomForest Classifier were tested. The best performing model was the Naive-Bayes Classifier, with a test accuracy of 0.904, specificity of 0.880 and  less overfitting. On new data, the accuracy is also high at 0.917 while specificity score peaked at 0.971.\n",
    "\n",
    "The final model can now be deployed with above 90% accuracy. To prevent false positives (i.e. post related to Religion being predicted as Spirituality), posts classified as Spirituality should be checked again should they contain keywords like phone, day, life, time and death as these are known to cause false positive misclassification by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Data Collection.\n",
    "Posts and discussions on the subjects of religion and spirituality will be collected from reddit.com. Using subreddits 'https://www.reddit.com/r/religion' and 'https://www.reddit.com/r/spirituality', data will be collected through reddit's API using the .json feeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T06:15:52.989143Z",
     "start_time": "2020-12-04T06:15:52.033953Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T06:15:56.995888Z",
     "start_time": "2020-12-04T06:15:56.985916Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the parameters needed\n",
    "headers = {'User-agent': 'Laz Inc 1.0'}\n",
    "subjects = ['religion','spirituality']\n",
    "areas = ['hot', 'controversial', 'new', 'top']\n",
    "periods = ['week','month', 'year', 'all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T06:18:15.241073Z",
     "start_time": "2020-12-04T06:16:38.420743Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'after': 't3_k5echi'}\n",
      "{'after': 't3_k4apfr'}\n",
      "{'after': 't3_k302sl'}\n",
      "{'after': 't3_k1i5q5'}\n",
      "{'after': 't3_jzzibs'}\n",
      "{'after': 't3_jydi7h'}\n",
      "{'after': 't3_jx19jc'}\n",
      "{'after': 't3_jvim5r'}\n",
      "{'after': 't3_ju511t'}\n",
      "None\n",
      "{'after': 't3_k626o4'}\n",
      "{'after': 't3_k5oz4a'}\n",
      "{'after': 't3_k56e22'}\n",
      "{'after': 't3_k4q3ol'}\n",
      "{'after': 't3_k4d6g1'}\n",
      "{'after': 't3_k3zy4h'}\n",
      "{'after': 't3_k3mu1s'}\n",
      "{'after': 't3_k37wkr'}\n",
      "{'after': 't3_k2r2v1'}\n"
     ]
    }
   ],
   "source": [
    "# subreddit json can be download using the url format below. we will repeat the api call for each subject, area and period\n",
    "# https://www.reddit.com/r/boardgames/top.json?t=all&limit=25\n",
    "for subject in subjects:\n",
    "    for area in areas:\n",
    "        for period in periods:\n",
    "            url = 'https://www.reddit.com/r/' + subject + '/' + area + '.json?t=' + period + '&limit=50'\n",
    "            after = None\n",
    "            filename = subject + '_' + area + '_' + period + '.csv'\n",
    "            for a in range(10):\n",
    "                if after == None:\n",
    "                    params = None\n",
    "                else:\n",
    "                    params = {'after':after}\n",
    "                print(params)\n",
    "                res = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "                if res.status_code != 200:\n",
    "                    print('Status error', res.status_code)\n",
    "                    break\n",
    "\n",
    "                current_dict = res.json()\n",
    "                current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "\n",
    "                # convert current posts into dataframe\n",
    "                current_df = pd.DataFrame(current_posts)\n",
    "\n",
    "                # get the after value\n",
    "                after = current_dict['data']['after']\n",
    "                if after == None:\n",
    "                    break\n",
    "\n",
    "                # add new posts to csv file\n",
    "                if a > 0:\n",
    "                    prev_posts = pd.read_csv('./csv_files_2/' + filename)\n",
    "                    pd.concat([prev_posts, current_df]).to_csv(filename, index = False)\n",
    "                else:\n",
    "                    current_df.to_csv('./csv_files_2/' + filename, index = False)\n",
    "\n",
    "                # generate a random sleep duration to look more 'natural'\n",
    "                sleep_duration = random.randint(2,5)\n",
    "                # print(sleep_duration)\n",
    "                time.sleep(sleep_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all csv files downloaded and drop duplicates and NAs.\n",
    "After downloading the json files and saving each download as a separate .csv file, we have 16 csv files in total for each subreddit.\n",
    "\n",
    "Next we will combined these 16 files into 1 single file for each subreddit, remove duplicate rows and any rows with nan data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-30T14:50:57.769939Z",
     "start_time": "2020-11-30T14:50:57.756974Z"
    }
   },
   "outputs": [],
   "source": [
    "subjects = ['religion', 'spirituality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-30T14:54:06.178899Z",
     "start_time": "2020-11-30T14:54:03.257273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./csv_files\\religion_controversial_all.csv\n",
      "./csv_files\\religion_controversial_month.csv\n",
      "./csv_files\\religion_controversial_week.csv\n",
      "./csv_files\\religion_controversial_year.csv\n",
      "./csv_files\\religion_hot.csv\n",
      "./csv_files\\religion_hot_all.csv\n",
      "./csv_files\\religion_hot_month.csv\n",
      "./csv_files\\religion_hot_week.csv\n",
      "./csv_files\\religion_hot_year.csv\n",
      "./csv_files\\religion_new.csv\n",
      "./csv_files\\religion_new_all.csv\n",
      "./csv_files\\religion_new_month.csv\n",
      "./csv_files\\religion_new_week.csv\n",
      "./csv_files\\religion_new_year.csv\n",
      "./csv_files\\religion_top_all.csv\n",
      "./csv_files\\religion_top_month.csv\n",
      "./csv_files\\religion_top_week.csv\n",
      "./csv_files\\religion_top_year.csv\n",
      "religion 1342\n",
      "./csv_files\\spirituality_controversial_all.csv\n",
      "./csv_files\\spirituality_controversial_month.csv\n",
      "./csv_files\\spirituality_controversial_week.csv\n",
      "./csv_files\\spirituality_controversial_year.csv\n",
      "./csv_files\\spirituality_hot_all.csv\n",
      "./csv_files\\spirituality_hot_month.csv\n",
      "./csv_files\\spirituality_hot_week.csv\n",
      "./csv_files\\spirituality_hot_year.csv\n",
      "./csv_files\\spirituality_new_all.csv\n",
      "./csv_files\\spirituality_new_month.csv\n",
      "./csv_files\\spirituality_new_week.csv\n",
      "./csv_files\\spirituality_new_year.csv\n",
      "./csv_files\\spirituality_top_all.csv\n",
      "./csv_files\\spirituality_top_month.csv\n",
      "./csv_files\\spirituality_top_week.csv\n",
      "./csv_files\\spirituality_top_year.csv\n",
      "spirituality 2377\n"
     ]
    }
   ],
   "source": [
    "# using glob to combine\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# loop for each subject\n",
    "for subject in subjects:\n",
    "    # create an empty dataframe for concatenation\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # loop for all csv files belonging to the subject\n",
    "    for name in glob.glob('./csv_files/' + subject + '*.csv'):\n",
    "        print(name)\n",
    "        # read in the csv file as dataframe\n",
    "        df_new = pd.read_csv(name)\n",
    "        \n",
    "        # concatenate the newly read dataframe with the df dataframe\n",
    "        df = pd.concat([df, df_new])\n",
    "    \n",
    "    # drop all duplicate rows and any rows with NAN values\n",
    "    df_clean = df[['subreddit', 'title','selftext','created_utc']].drop_duplicates(subset=['subreddit', 'title','selftext']).dropna()\n",
    "    \n",
    "    # save the combined data for each subreddit \n",
    "    df_clean.to_csv(subject + '_cleaned.csv', index=False)\n",
    "    \n",
    "    # print out the number of rows saved for each subreddit\n",
    "    print(subject, len(df_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is ready, [continue to classification](classification.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
